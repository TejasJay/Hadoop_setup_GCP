## **üìå Full Hadoop & Hive Setup Notes**

This document contains **detailed step-by-step notes** explaining **everything we did today**, covering:
‚úî **Hadoop installation & setup**
‚úî **Configuring HDFS & YARN**
‚úî **Hive installation & setup**
‚úî **Query execution & data ingestion**
‚úî **Full automation of the pipeline**


* * *

# **üöÄ Part 1: Hadoop Installation & Setup**

Hadoop is the **core framework** that allows us to store and process **large-scale data** using a distributed system.

* * *

## **üõ† Step 1: Install Hadoop on the Master Node**

### **1Ô∏è‚É£ SSH into the Hadoop Master Node**

sh

CopyEdit

`gcloud compute ssh hadoop-master`

### **2Ô∏è‚É£ Download & Install Hadoop**

sh

CopyEdit

`wget https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz`

‚úî This command downloads **Hadoop version 3.3.6**.

### **3Ô∏è‚É£ Extract & Move Hadoop**

sh

CopyEdit

`tar -xvzf hadoop-3.3.6.tar.gz sudo mv hadoop-3.3.6 /usr/local/hadoop`

‚úî **Extracts the Hadoop package and moves it** to `/usr/local/hadoop`.

* * *

## **üõ† Step 2: Configure Hadoop Environment Variables**

We need to set up **Hadoop paths** so it can be accessed system-wide.

### **1Ô∏è‚É£ Edit `.bashrc` File**

sh

CopyEdit

`nano ~/.bashrc`

### **2Ô∏è‚É£ Add the Following Environment Variables**

sh

CopyEdit

`export HADOOP_HOME=/usr/local/hadoop export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64`

‚úî **Defines paths for Hadoop and Java**.

### **3Ô∏è‚É£ Apply Changes**

sh

CopyEdit

`source ~/.bashrc`

‚úî Ensures the new environment variables are active.

* * *

## **üõ† Step 3: Configure Hadoop Core Settings**

Hadoop requires multiple configuration changes.

### **1Ô∏è‚É£ Edit `core-site.xml`**

sh

CopyEdit

`nano /usr/local/hadoop/etc/hadoop/core-site.xml`

üîπ **Update the following properties:**

xml

CopyEdit

`<configuration>     <property>         <name>fs.defaultFS</name>         <value>hdfs://hadoop-master:9000</value>     </property> </configuration>`

‚úî **Defines Hadoop‚Äôs filesystem (HDFS) URL**.

* * *

### **2Ô∏è‚É£ Edit `hdfs-site.xml` (HDFS Configuration)**

sh

CopyEdit

`nano /usr/local/hadoop/etc/hadoop/hdfs-site.xml`

üîπ **Add replication and block size settings:**

xml

CopyEdit

`<configuration>     <property>         <name>dfs.replication</name>         <value>3</value>  <!-- Ensures data is replicated on multiple nodes -->     </property>     <property>         <name>dfs.blocksize</name>         <value>268435456</value>  <!-- 256MB Block Size -->     </property> </configuration>`

‚úî **Configures how Hadoop distributes and stores data.**

* * *

### **3Ô∏è‚É£ Edit `yarn-site.xml` (YARN Configuration)**

sh

CopyEdit

`nano /usr/local/hadoop/etc/hadoop/yarn-site.xml`

üîπ **Update the following properties:**

xml

CopyEdit

`<configuration>     <property>         <name>yarn.nodemanager.resource.memory-mb</name>         <value>8192</value>     </property>     <property>         <name>yarn.scheduler.minimum-allocation-mb</name>         <value>1024</value>     </property>     <property>         <name>yarn.scheduler.maximum-allocation-mb</name>         <value>8192</value>     </property> </configuration>`

‚úî **Allocates memory for YARN (Job Scheduler)**.

* * *

## **üõ† Step 4: Configure Worker Nodes**

Worker nodes execute the actual **MapReduce tasks**.

### **1Ô∏è‚É£ Edit the `workers` File**

sh

CopyEdit

`nano /usr/local/hadoop/etc/hadoop/workers`

üîπ **Add the following worker node hostnames:**

nginx

CopyEdit

`localhost hadoop-worker-1 hadoop-worker-2`

‚úî **Defines the worker nodes participating in the Hadoop cluster**.

* * *

## **üõ† Step 5: Set Up SSH Access Between Nodes**

Hadoop needs **password-less SSH access** to communicate between nodes.

### **1Ô∏è‚É£ Generate SSH Keys**

sh

CopyEdit

`ssh-keygen -t rsa -P "" -f ~/.ssh/id_rsa`

‚úî **Creates an SSH key without a passphrase**.

### **2Ô∏è‚É£ Copy SSH Key to Worker Nodes**

sh

CopyEdit

`ssh-copy-id tejasjay94@hadoop-worker-1 ssh-copy-id tejasjay94@hadoop-worker-2`

‚úî **Allows the master node to SSH into worker nodes without a password**.

* * *

## **üõ† Step 6: Format the Hadoop Filesystem**

HDFS must be formatted before first use.

sh

CopyEdit

`hdfs namenode -format`

‚úî **Prepares Hadoop's distributed storage system (HDFS) for use**.

* * *

## **üõ† Step 7: Start Hadoop Services**

Hadoop has multiple services that need to be started.

### **1Ô∏è‚É£ Start HDFS (Hadoop Distributed File System)**

sh

CopyEdit

`start-dfs.sh`

### **2Ô∏è‚É£ Start YARN (Resource Manager)**

sh

CopyEdit

`start-yarn.sh`

‚úî **These commands start Hadoop's storage and job scheduling services.**

* * *

## **üõ† Step 8: Verify Hadoop is Running**

Check that all **Hadoop services** are running:

sh

CopyEdit

`jps`

‚úÖ **Expected Output:**

CopyEdit

`12345 NameNode 12346 DataNode 12347 SecondaryNameNode 12348 ResourceManager 12349 NodeManager 12350 Jps`

‚úî **Indicates all core Hadoop services are running successfully.**

* * *

# **üöÄ Part 2: Apache Hive Setup & Configuration**

Hive is a **data warehouse system** that allows us to **run SQL-like queries on Hadoop**.

* * *

## **üõ† Step 9: Install Hive**

### **1Ô∏è‚É£ Download & Install Hive**

sh

CopyEdit

`wget https://archive.apache.org/dist/hive/hive-3.1.3/apache-hive-3.1.3-bin.tar.gz tar -xvzf apache-hive-3.1.3-bin.tar.gz sudo mv apache-hive-3.1.3-bin /usr/local/hive`

‚úî **Extracts Hive and moves it to the correct directory.**

### **2Ô∏è‚É£ Set Hive Environment Variables**

sh

CopyEdit

`nano ~/.bashrc`

üîπ **Add:**

sh

CopyEdit

`export HIVE_HOME=/usr/local/hive export PATH=$PATH:$HIVE_HOME/bin`

‚úî **Ensures Hive commands work system-wide**.

### **3Ô∏è‚É£ Apply Changes**

sh

CopyEdit

`source ~/.bashrc`

* * *

## **üõ† Step 10: Configure Hive Metastore**

Hive stores metadata in a **Metastore Database**.

### **1Ô∏è‚É£ Edit `hive-site.xml`**

sh

CopyEdit

`nano /usr/local/hive/conf/hive-site.xml`

üîπ **Add database connection settings:**

xml

CopyEdit

`<configuration>     <property>         <name>javax.jdo.option.ConnectionURL</name>         <value>jdbc:derby:;databaseName=/usr/local/hive/metastore_db;create=true</value>     </property>     <property>         <name>hive.metastore.warehouse.dir</name>         <value>/user/hive/warehouse</value>     </property> </configuration>`

‚úî **Configures Hive to store metadata using Apache Derby.**

* * *

## **üõ† Step 11: Initialize Hive Metastore**

sh

CopyEdit

`schematool -initSchema -dbType derby`

‚úî **Prepares Hive's internal database for storing metadata.**

* * *

**üìå Next Up: Hive Query Execution, Data Ingestion & Full Automation!**
(Continuing in next response‚Ä¶) üöÄ

4o

##### You said:

continue

###### ChatGPT said:

## **üìå Part 3: Running Hive Queries & Data Ingestion**

Now that Hive is set up, we will:
‚úî **Create databases and tables in Hive**
‚úî **Load data from HDFS into Hive**
‚úî **Run SQL-like queries to analyze data**

* * *

## **üõ† Step 12: Start Hive & Create a Database**

### **1Ô∏è‚É£ Start Hive CLI**

sh

CopyEdit

`hive`

‚úî **Opens the Hive interactive command-line interface.**

### **2Ô∏è‚É£ Create a New Database**

sql

CopyEdit

`CREATE DATABASE nyctaxi;`

‚úî **Creates a database to store NYC taxi trip data.**

### **3Ô∏è‚É£ Switch to the New Database**

sql

CopyEdit

`USE nyctaxi;`

‚úî **Ensures all tables are created under this database.**

* * *

## **üõ† Step 13: Create Hive Table for NYC Taxi Data**

Since our dataset is in **CSV format**, we need a **Hive table** to store structured data.

### **1Ô∏è‚É£ Create a Table**

sql

CopyEdit

`CREATE TABLE yellow_taxi (     VendorID INT,     tpep_pickup_datetime STRING,     tpep_dropoff_datetime STRING,     passenger_count INT,     trip_distance FLOAT,     fare_amount FLOAT,     total_amount FLOAT,     payment_type INT ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE;`

‚úî **Defines a structured table for NYC taxi data.**

* * *

## **üõ† Step 14: Upload CSV Data to HDFS**

Before loading data into Hive, we must first **move it into HDFS**.

### **1Ô∏è‚É£ Create a Directory in HDFS**

sh

CopyEdit

`hdfs dfs -mkdir -p /user/hive/warehouse/nyc_taxi/`

‚úî **Creates an HDFS location for storing taxi data.**

### **2Ô∏è‚É£ Upload the NYC Taxi CSV File**

sh

CopyEdit

`hdfs dfs -put -f yellow_tripdata_2025-03.csv /user/hive/warehouse/nyc_taxi/`

‚úî **Transfers the CSV file to HDFS.**

* * *

## **üõ† Step 15: Load Data into Hive**

### **1Ô∏è‚É£ Load NYC Taxi Data from HDFS into Hive**

sql

CopyEdit

`LOAD DATA INPATH '/user/hive/warehouse/nyc_taxi/yellow_tripdata_2025-03.csv' INTO TABLE yellow_taxi;`

‚úî **Imports data from HDFS into Hive for querying.**

### **2Ô∏è‚É£ Verify Data in Hive**

sql

CopyEdit

`SELECT * FROM yellow_taxi LIMIT 10;`

‚úÖ **Expected Output:**

yaml

CopyEdit

`1  2025-03-01 00:10:00  2025-03-01 00:20:00  1  2.5  12.00  15.50  1 1  2025-03-01 01:30:00  2025-03-01 01:45:00  2  5.0  20.00  23.00  2 ...`

* * *

## **üõ† Step 16: Run Analytics Queries in Hive**

Hive allows us to run **SQL-like queries** for **big data analysis**.

### **1Ô∏è‚É£ Count Total Number of Trips**

sql

CopyEdit

`SELECT COUNT(*) FROM yellow_taxi;`

‚úî **Returns the total number of taxi trips.**

### **2Ô∏è‚É£ Find the Average Fare**

sql

CopyEdit

`SELECT AVG(fare_amount) FROM yellow_taxi;`

‚úî **Calculates the average fare per trip.**

### **3Ô∏è‚É£ Find the Busiest Pickup Hours**

sql

CopyEdit

`SELECT HOUR(tpep_pickup_datetime) AS pickup_hour, COUNT(*) AS trip_count FROM yellow_taxi GROUP BY HOUR(tpep_pickup_datetime) ORDER BY trip_count DESC LIMIT 5;`

‚úî **Determines the busiest times for taxis.**

### **4Ô∏è‚É£ Order Trips by Fare Amount**

sql

CopyEdit

`SELECT * FROM yellow_taxi ORDER BY fare_amount DESC LIMIT 10;`

‚úî **Lists the most expensive taxi trips.**

* * *

## **üìå Part 4: Automating Data Pipeline**

Now that we have manually processed **Parquet ‚Üí CSV ‚Üí HDFS ‚Üí Hive**, we will fully **automate the entire workflow**.

* * *

## **üõ† Step 17: Automating Data Ingestion**

We will create a **shell script** that:
‚úî **Downloads new NYC taxi data monthly**
‚úî **Converts Parquet to CSV**
‚úî **Uploads CSV to HDFS**

### **1Ô∏è‚É£ Create the Script (`data_ingestion.sh`)**

sh

CopyEdit

`nano data_ingestion.sh`

### **2Ô∏è‚É£ Add the Following Script**

sh

CopyEdit

`#!/bin/bash  LOG_FILE="/home/tejasjay94/logs/data_pipeline.log" CURRENT_YEAR=$(date +'%Y') CURRENT_MONTH=$(date +'%m')  DATA_URL="https://www.nyc.gov/assets/tlc/downloads/yellow_tripdata_${CURRENT_YEAR}-${CURRENT_MONTH}.parquet" LOCAL_FILE="yellow_tripdata_${CURRENT_YEAR}-${CURRENT_MONTH}.parquet" CSV_FILE="yellow_tripdata_${CURRENT_YEAR}-${CURRENT_MONTH}.csv" HDFS_DIR="/user/hive/warehouse/nyc_taxi/"  echo "üöÄ Starting Data Pipeline - $(date)" | tee -a $LOG_FILE  # Step 1: Download Latest Taxi Data echo "üì• Downloading $DATA_URL..." | tee -a $LOG_FILE wget -O "$LOCAL_FILE" "$DATA_URL"  # Step 2: Convert Parquet to CSV echo "üõ† Converting Parquet to CSV..." | tee -a $LOG_FILE python3 convert_parquet_to_csv.py "$LOCAL_FILE"  # Step 3: Upload to HDFS echo "üì§ Uploading to HDFS..." | tee -a $LOG_FILE hdfs dfs -put -f "$CSV_FILE" "$HDFS_DIR"  echo "‚úÖ Data Pipeline Completed Successfully!" | tee -a $LOG_FILE`

### **3Ô∏è‚É£ Make the Script Executable**

sh

CopyEdit

`chmod +x data_ingestion.sh`

* * *

## **üõ† Step 18: Automating Hive Queries**

We will now **automate table creation and incremental data load**.

### **1Ô∏è‚É£ Create the Script (`run_hive_analysis.sh`)**

sh

CopyEdit

`nano run_hive_analysis.sh`

### **2Ô∏è‚É£ Add the Following Script**

sh

CopyEdit

`#!/bin/bash  LOG_FILE="/home/tejasjay94/logs/hive_queries.log" CURRENT_YEAR=$(date +'%Y') CURRENT_MONTH=$(date +'%m')  CSV_FILE="/user/hive/warehouse/nyc_taxi/yellow_tripdata_${CURRENT_YEAR}-${CURRENT_MONTH}.csv" HIVE_QUERY_FILE="/home/tejasjay94/hive_queries_dynamic.sql"  cat <<EOF > $HIVE_QUERY_FILE USE nyctaxi;  CREATE TABLE IF NOT EXISTS yellow_taxi (     VendorID INT,     tpep_pickup_datetime STRING,     tpep_dropoff_datetime STRING,     passenger_count INT,     trip_distance FLOAT,     fare_amount FLOAT,     total_amount FLOAT,     payment_type INT ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE;  CREATE TABLE IF NOT EXISTS yellow_taxi_staging LIKE yellow_taxi;  LOAD DATA INPATH '${CSV_FILE}' INTO TABLE yellow_taxi_staging;  INSERT INTO TABLE yellow_taxi SELECT * FROM yellow_taxi_staging WHERE tpep_pickup_datetime NOT IN (SELECT tpep_pickup_datetime FROM yellow_taxi);  DROP TABLE yellow_taxi_staging; EOF  hive -f $HIVE_QUERY_FILE >> $LOG_FILE 2>&1`

### **3Ô∏è‚É£ Make the Script Executable**

sh

CopyEdit

`chmod +x run_hive_analysis.sh`

* * *

## **üõ† Step 19: Automate Execution with Cron Jobs**

### **1Ô∏è‚É£ Open Cron Job Editor**

sh

CopyEdit

`crontab -e`

### **2Ô∏è‚É£ Add Scheduled Jobs**

sh

CopyEdit

`0 2 1 * * /home/tejasjay94/data_ingestion.sh 30 2 1 * * /home/tejasjay94/run_hive_analysis.sh`

‚úî **Runs the data ingestion & Hive analysis on the 1st of every month.**

* * *
